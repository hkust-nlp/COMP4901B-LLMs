========================================
GSM8K Evaluation Pipeline
========================================
Model: Qwen-0.6B
Output Directory: results/baseline/multiple_rollout
Run Name: Qwen-0.6B_20251117_093126
Dataset Split: test
Number of Queries: 1000
Max Tokens: 512
Temperature: 0.6
Top-p: 0.95
Top-k: 20
Number of Rollouts: 8
Tensor Parallel: 1
Mode: Zero-shot
Chat Template: Enabled
Thinking Mode: Disabled
========================================

[1/2] Running inference...
Output will be saved to: results/baseline/multiple_rollout/Qwen-0.6B_20251117_093126_inference.jsonl

Running: python inference_vllm.py     --model_path "Qwen-0.6B"     --output_path "results/baseline/multiple_rollout/Qwen-0.6B_20251117_093126_inference.jsonl"     --max_tokens 512     --temperature 0.6     --top_p 0.95     --top_k 20     --n_rollouts 8     --tensor_parallel_size 1     --gpu_memory_utilization 0.8     --split test     --n_queries 1000
INFO 11-17 09:31:33 [__init__.py:216] Automatically detected platform cuda.
2025-11-17 09:31:36,384 - INFO - ================================================================================
2025-11-17 09:31:36,384 - INFO - Starting VLLM Inference for GSM8K
2025-11-17 09:31:36,384 - INFO - ================================================================================
2025-11-17 09:31:36,385 - INFO - Model path: Qwen-0.6B
2025-11-17 09:31:36,385 - INFO - Output path: results/baseline/multiple_rollout/Qwen-0.6B_20251117_093126_inference.jsonl
2025-11-17 09:31:36,385 - INFO - Dataset split: test
2025-11-17 09:31:36,385 - INFO - Mode: Zero-shot
2025-11-17 09:31:36,385 - INFO - Chat template: Enabled
2025-11-17 09:31:36,385 - INFO - Thinking mode: Disabled
2025-11-17 09:31:36,385 - INFO - Tensor parallel size: 1
2025-11-17 09:31:36,385 - INFO - Max tokens: 512
2025-11-17 09:31:36,385 - INFO - Temperature: 0.6
2025-11-17 09:31:36,385 - INFO - Top-p: 0.95
2025-11-17 09:31:36,385 - INFO - Top-k: 20
2025-11-17 09:31:36,385 - INFO - Number of rollouts per question: 8
2025-11-17 09:31:36,385 - INFO - Loading GSM8K dataset (split: test)...
2025-11-17 09:31:36,519 - INFO - Loaded 1319 examples from GSM8K test set
2025-11-17 09:31:36,521 - INFO - Using subset of 1000 queries (from 1319 total)
2025-11-17 09:31:36,560 - INFO - Loading tokenizer from Qwen-0.6B
Traceback (most recent call last):
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 402, in hf_raise_for_status
    response.raise_for_status()
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/Qwen-0.6B/resolve/main/tokenizer_config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1114, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1655, in _raise_on_head_call_error
    raise head_call_error
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1543, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata
    r = _request_wrapper(
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper
    response = _request_wrapper(
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper
    hf_raise_for_status(response)
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 452, in hf_raise_for_status
    raise _format(RepositoryNotFoundError, message, response) from e
huggingface_hub.errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-691a7af8-392733811f772fa87bae61c5;6c144da7-be5d-4f17-80fd-c2b8977c3b27)

Repository Not Found for url: https://huggingface.co/Qwen-0.6B/resolve/main/tokenizer_config.json.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication
Invalid username or password.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/store/comp4901b/yxiao986/COMP4901B-LLMs/assignment3/inference_vllm.py", line 438, in <module>
    main()
  File "/store/comp4901b/yxiao986/COMP4901B-LLMs/assignment3/inference_vllm.py", line 417, in main
    run_inference(
  File "/store/comp4901b/yxiao986/COMP4901B-LLMs/assignment3/inference_vllm.py", line 229, in run_inference
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 1073, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 905, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/transformers/utils/hub.py", line 511, in cached_files
    raise OSError(
OSError: Qwen-0.6B is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`
ERROR: Inference failed - output file not created
