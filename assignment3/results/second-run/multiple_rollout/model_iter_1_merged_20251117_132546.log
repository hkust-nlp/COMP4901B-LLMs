========================================
GSM8K Evaluation Pipeline
========================================
Model: ckpt/second-run/models/model_iter_1_merged
Output Directory: results/second-run/multiple_rollout
Run Name: model_iter_1_merged_20251117_132546
Dataset Split: test
Number of Queries: 1000
Max Tokens: 512
Temperature: 0.6
Top-p: 0.95
Top-k: 20
Number of Rollouts: 8
Tensor Parallel: 1
Mode: Zero-shot
Chat Template: Enabled
Thinking Mode: Disabled
========================================

[1/2] Running inference...
Output will be saved to: results/second-run/multiple_rollout/model_iter_1_merged_20251117_132546_inference.jsonl

Running: python inference_vllm.py     --model_path "ckpt/second-run/models/model_iter_1_merged"     --output_path "results/second-run/multiple_rollout/model_iter_1_merged_20251117_132546_inference.jsonl"     --max_tokens 512     --temperature 0.6     --top_p 0.95     --top_k 20     --n_rollouts 8     --tensor_parallel_size 1     --gpu_memory_utilization 0.8     --split test     --n_queries 1000
INFO 11-17 13:25:54 [__init__.py:216] Automatically detected platform cuda.
2025-11-17 13:25:57,276 - INFO - ================================================================================
2025-11-17 13:25:57,277 - INFO - Starting VLLM Inference for GSM8K
2025-11-17 13:25:57,277 - INFO - ================================================================================
2025-11-17 13:25:57,277 - INFO - Model path: ckpt/second-run/models/model_iter_1_merged
2025-11-17 13:25:57,277 - INFO - Output path: results/second-run/multiple_rollout/model_iter_1_merged_20251117_132546_inference.jsonl
2025-11-17 13:25:57,277 - INFO - Dataset split: test
2025-11-17 13:25:57,277 - INFO - Mode: Zero-shot
2025-11-17 13:25:57,277 - INFO - Chat template: Enabled
2025-11-17 13:25:57,277 - INFO - Thinking mode: Disabled
2025-11-17 13:25:57,277 - INFO - Tensor parallel size: 1
2025-11-17 13:25:57,278 - INFO - Max tokens: 512
2025-11-17 13:25:57,278 - INFO - Temperature: 0.6
2025-11-17 13:25:57,278 - INFO - Top-p: 0.95
2025-11-17 13:25:57,278 - INFO - Top-k: 20
2025-11-17 13:25:57,278 - INFO - Number of rollouts per question: 8
2025-11-17 13:25:57,278 - INFO - Loading GSM8K dataset (split: test)...
2025-11-17 13:25:57,407 - INFO - Loaded 1319 examples from GSM8K test set
2025-11-17 13:25:57,409 - INFO - Using subset of 1000 queries (from 1319 total)
2025-11-17 13:25:57,447 - INFO - Loading tokenizer from ckpt/second-run/models/model_iter_1_merged
Traceback (most recent call last):
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/transformers/utils/hub.py", line 479, in cached_files
    hf_hub_download(
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'ckpt/second-run/models/model_iter_1_merged'. Use `repo_type` argument if needed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/store/comp4901b/yxiao986/COMP4901B-LLMs/assignment3/inference_vllm.py", line 438, in <module>
    main()
  File "/store/comp4901b/yxiao986/COMP4901B-LLMs/assignment3/inference_vllm.py", line 417, in main
    run_inference(
  File "/store/comp4901b/yxiao986/COMP4901B-LLMs/assignment3/inference_vllm.py", line 229, in run_inference
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 1073, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 905, in get_tokenizer_config
    resolved_config_file = cached_file(
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/transformers/utils/hub.py", line 322, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/transformers/utils/hub.py", line 531, in cached_files
    resolved_files = [
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/transformers/utils/hub.py", line 532, in <listcomp>
    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
    resolved_file = try_to_load_from_cache(
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/store/comp4901b/yxiao986/miniconda3/envs/hw3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'ckpt/second-run/models/model_iter_1_merged'. Use `repo_type` argument if needed.
ERROR: Inference failed - output file not created
